{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "996a927b-be61-410b-8535-bc0adf830169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de454f-bbfd-4338-ab47-ff0728f2cd2d",
   "metadata": {},
   "source": [
    "## Get all product links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d765b40d-1278-471b-a4eb-b95c51705798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Start Time: 15:45:08.686772 ---------------------------> \n",
      "Waiting for search input...\n",
      "Typing in search input...\n",
      "Submitting search form...\n",
      "Waiting for search results...\n",
      "Collecting pagination links...\n",
      "Pagination Links Count: 25\n",
      "All Pagination Links:  ['https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=1', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=2', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=3', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=4', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=5', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=6', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=7', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=8', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=9', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=10', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=11', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=12', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=13', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=14', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=15', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=16', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=17', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=18', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=19', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=20', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=21', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=22', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=23', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=24', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=25']\n",
      "Collecting Product Detail Page Links\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=1 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=2 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=3 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=4 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=5 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=6 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=7 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=8 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=9 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=10 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=11 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=12 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=13 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=14 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=15 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=16 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=17 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=18 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=19 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=20 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=21 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=22 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=23 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=24 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=25 Done ------>\n",
      "All Product Detail Page Links Captured:  1000\n",
      "Total Product Detail Page Links 1000\n",
      "Session End Time: 15:46:46.668420 ---------------------------> \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script uses Selenium WebDriver to automate searching for \"sports shoes for women\" on Flipkart,\n",
    "collects pagination links for the first 25 pages, extracts product detail page links,\n",
    "and saves them into a CSV file.\n",
    "\n",
    "Key Steps:\n",
    "- Start Chrome browser session.\n",
    "- Search for a query.\n",
    "- Navigate through paginated result pages.\n",
    "- Scrape product detail page URLs.\n",
    "- Save all collected links to a CSV.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Define search query and website link\n",
    "search_box_text = 'sports shoes for women'\n",
    "website_link = 'https://www.flipkart.com/'\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Start the browser session and note session start time\n",
    "# ------------------------------------------------------------\n",
    "session_start_time = datetime.now().time()\n",
    "print(f\"Session Start Time: {session_start_time} ---------------------------> \")\n",
    "\n",
    "# Initialize Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to Flipkart's website\n",
    "driver.get(website_link)\n",
    "\n",
    "# Maximize the browser window for better visibility and interaction\n",
    "driver.maximize_window()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Locate the search input box and enter search term\n",
    "# ------------------------------------------------------------\n",
    "print('Waiting for search input...')\n",
    "\n",
    "# Wait up to 120 seconds for the search input box to be present\n",
    "search_input = WebDriverWait(driver, 120).until(\n",
    "    EC.presence_of_element_located((By.CSS_SELECTOR, '[autocomplete=\"off\"]'))\n",
    ")\n",
    "\n",
    "print('Typing in search input...')\n",
    "\n",
    "# Enter the defined search term into the search box\n",
    "search_input.send_keys(search_box_text)\n",
    "\n",
    "print('Submitting search form...')\n",
    "\n",
    "# Simulate pressing Enter to submit the search form\n",
    "search_input.send_keys(Keys.RETURN)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Wait for the search results page to load\n",
    "# ------------------------------------------------------------\n",
    "print('Waiting for search results...')\n",
    "\n",
    "# Wait until at least one product link (which opens in new tab) appears on page\n",
    "WebDriverWait(driver, 120).until(\n",
    "    EC.presence_of_element_located((By.CSS_SELECTOR, '[target=\"_blank\"]'))\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Collect pagination links for first 25 pages\n",
    "# ------------------------------------------------------------\n",
    "print('Collecting pagination links...')\n",
    "\n",
    "\"\"\"\n",
    "Logic:\n",
    "- Get the link for the first page from the pagination bar.\n",
    "- Derive links for pages 2 to 25 by modifying the page number in the URL.\n",
    "\"\"\"\n",
    "\n",
    "all_pagination_links = []\n",
    "\n",
    "# Find the first pagination link\n",
    "first_page = driver.find_elements(By.CSS_SELECTOR, 'nav a')[0]\n",
    "first_page_link = first_page.get_attribute('href')\n",
    "\n",
    "# Add first page link to the list\n",
    "all_pagination_links.append(first_page_link)\n",
    "\n",
    "# Generate links for pages 2 to 25 by modifying the page number at the end\n",
    "for i in range(2, 26):\n",
    "    new_pagination_link = first_page_link[:-1] + str(i)\n",
    "    all_pagination_links.append(new_pagination_link)\n",
    "\n",
    "print('Pagination Links Count:', len(all_pagination_links))\n",
    "print('All Pagination Links: ', all_pagination_links)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Visit each pagination link and collect product detail page links\n",
    "# ------------------------------------------------------------\n",
    "print('Collecting Product Detail Page Links')\n",
    "all_product_links = []\n",
    "\n",
    "# Loop through each pagination link\n",
    "for link in all_pagination_links:\n",
    "    driver.get(link)\n",
    "    \n",
    "    # Wait until page is fully loaded\n",
    "    WebDriverWait(driver, 120).until(\n",
    "        lambda d: d.execute_script('return document.readyState') == 'complete'\n",
    "    )\n",
    "    \n",
    "    # Wait until product elements with class 'rPDeLR' are present\n",
    "    WebDriverWait(driver, 120).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, 'rPDeLR'))\n",
    "    )\n",
    "    \n",
    "    # Find all product link elements on the page\n",
    "    all_products = driver.find_elements(By.CLASS_NAME, 'rPDeLR')\n",
    "    \n",
    "    # Extract the 'href' attributes (product detail page URLs)\n",
    "    all_links = [element.get_attribute('href') for element in all_products]\n",
    "    \n",
    "    print(f\"{link} Done ------>\")\n",
    "    \n",
    "    # Add collected links to the master list\n",
    "    all_product_links.extend(all_links)\n",
    "\n",
    "print('All Product Detail Page Links Captured: ', len(all_product_links))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. Create DataFrame, remove duplicates, and save to CSV\n",
    "# ------------------------------------------------------------\n",
    "# Create a pandas DataFrame from the list of product URLs\n",
    "df_product_links = pd.DataFrame(all_product_links, columns=['product_links'])\n",
    "\n",
    "# Remove any duplicate URLs to ensure uniqueness\n",
    "df_product_links = df_product_links.drop_duplicates(subset=['product_links'])\n",
    "\n",
    "print('Total Unique Product Detail Page Links', len(df_product_links))\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "df_product_links.to_csv('flipkart_product_links.csv', index=False)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. Close the browser session and record session end time\n",
    "# ------------------------------------------------------------\n",
    "driver.close()\n",
    "session_end_time = datetime.now().time()\n",
    "print(f\"Session End Time: {session_end_time} ---------------------------> \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a63bf6-798c-4afb-bbfe-6bd7e6c5f239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "033c8a8c-fece-4c1e-907d-a0d8957df813",
   "metadata": {},
   "source": [
    "## Get individual product information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff536cf8-2cef-4930-b9d4-0622cb8ba9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Start Time: 15:57:06.678946 ---------------------------> \n",
      "Collecting Individual Product Detail Information\n",
      "URL 1 completed *******\n",
      "URL 2 completed *******\n",
      "URL 3 completed *******\n",
      "URL 4 completed *******\n",
      "URL 5 completed *******\n",
      "URL 6 completed *******\n",
      "URL 7 completed *******\n",
      "URL 8 completed *******\n",
      "URL 9 completed *******\n",
      "URL 10 completed *******\n",
      "Total product pages scrapped:  10\n",
      "Final Total Products:  10\n",
      "Total Unavailable Products :  0\n",
      "Total Duplicate Products:  0\n",
      "Session End Time: 15:57:24.174974 ---------------------------> \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script reads a CSV containing Flipkart product detail page links,\n",
    "visits each product page using Selenium WebDriver, extracts product information\n",
    "(brand, title, price, discount, ratings), handles unavailable products,\n",
    "removes duplicates, and saves the results to CSV files.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Start the scraping session and log session start time\n",
    "# ------------------------------------------------------------\n",
    "session_start_time = datetime.now().time()\n",
    "print(f\"Session Start Time: {session_start_time} ---------------------------> \")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Read the CSV file containing all collected product links\n",
    "# ------------------------------------------------------------\n",
    "df_product_links = pd.read_csv(\"flipkart_product_links.csv\")\n",
    "\n",
    "# For demonstration purposes, limit to first 10 products.\n",
    "# Remove the below line to scrape all products.\n",
    "df_product_links = df_product_links.head(10)\n",
    "\n",
    "# Convert DataFrame column to a list of URLs\n",
    "all_product_links = df_product_links['product_links'].tolist()\n",
    "print(\"Collecting Individual Product Detail Information\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Start the Selenium WebDriver\n",
    "# ------------------------------------------------------------\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Initialize lists and counters for storing results\n",
    "complete_product_details = []\n",
    "unavailable_products = []\n",
    "successful_parsed_urls_count = 0\n",
    "complete_failed_urls_count = 0\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Loop through each product page link and scrape details\n",
    "# ------------------------------------------------------------\n",
    "for product_page_link in all_product_links:\n",
    "    try:\n",
    "        # Navigate to the product page\n",
    "        driver.get(product_page_link)\n",
    "\n",
    "        # Wait for the page to fully load\n",
    "        WebDriverWait(driver, 120).until(\n",
    "            lambda d: d.execute_script('return document.readyState') == 'complete'\n",
    "        )\n",
    "\n",
    "        # Wait until at least one targetable link is present\n",
    "        WebDriverWait(driver, 120).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, '[target=\"_blank\"]'))\n",
    "        )\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # 4a. Check if the product is marked as unavailable\n",
    "        # ----------------------------------------------------\n",
    "        try:\n",
    "            product_status = driver.find_element(By.CLASS_NAME, 'Z8JjpR').text\n",
    "            if product_status in ['Currently Unavailable', 'Sold Out']:\n",
    "                unavailable_products.append(product_page_link)\n",
    "                successful_parsed_urls_count += 1\n",
    "                print(f\"URL {successful_parsed_urls_count} completed --->\")\n",
    "                continue\n",
    "        except:\n",
    "            # No status means the product is likely available; continue\n",
    "            pass\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # 4b. Extract product details\n",
    "        # ----------------------------------------------------\n",
    "\n",
    "        # Brand\n",
    "        brand = driver.find_element(By.CLASS_NAME, 'mEh187').text\n",
    "\n",
    "        # Title - remove parenthetical color info\n",
    "        title = driver.find_element(By.CLASS_NAME, 'VU-ZEz').text\n",
    "        title = re.sub(r'\\s*\\([^)]*\\)', '', title)\n",
    "\n",
    "        # Price - extract digits only\n",
    "        price = driver.find_element(By.CLASS_NAME, 'Nx9bqj').text\n",
    "        price = ''.join(re.findall(r'\\d+', price))\n",
    "\n",
    "        # Discount - optional field, may be missing\n",
    "        try:\n",
    "            discount_text = driver.find_element(By.CLASS_NAME, 'UkUFwK').text\n",
    "            discount_numbers = re.findall(r'\\d+', discount_text)\n",
    "            discount = ''.join(discount_numbers)\n",
    "            discount = int(discount) / 100\n",
    "        except:\n",
    "            discount = ''\n",
    "\n",
    "        # Ratings and Reviews - optional fields\n",
    "        try:\n",
    "            product_review_status = driver.find_element(By.CLASS_NAME, 'E3XX7J').text\n",
    "            if product_review_status == 'Be the first to Review this product':\n",
    "                avg_rating = ''\n",
    "                total_ratings = ''\n",
    "        except:\n",
    "            try:\n",
    "                avg_rating = driver.find_element(By.CLASS_NAME, 'XQDdHH').text\n",
    "                total_ratings = driver.find_element(By.CLASS_NAME, 'Wphh3N').text.split(' ')[0]\n",
    "                # Remove commas in ratings count\n",
    "                total_ratings = int(total_ratings.replace(',', '')) if ',' in total_ratings else int(total_ratings)\n",
    "            except:\n",
    "                avg_rating = ''\n",
    "                total_ratings = ''\n",
    "\n",
    "        # Append the collected data to the results list\n",
    "        complete_product_details.append([\n",
    "            product_page_link, title, brand, price, discount, avg_rating, total_ratings\n",
    "        ])\n",
    "\n",
    "        # Increment and log success counter\n",
    "        successful_parsed_urls_count += 1\n",
    "        print(f\"URL {successful_parsed_urls_count} completed *******\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any failures in accessing or parsing the page\n",
    "        print(f\"Failed to establish a connection for URL {product_page_link}:  {e}\")\n",
    "        unavailable_products.append(product_page_link)\n",
    "        complete_failed_urls_count += 1\n",
    "        print(f\"Failed URL Count {complete_failed_urls_count}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Create pandas DataFrames for results\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# DataFrame for successfully scraped product details\n",
    "df = pd.DataFrame(\n",
    "    complete_product_details,\n",
    "    columns=['product_link', 'title', 'brand', 'price', 'discount', 'avg_rating', 'total_ratings']\n",
    ")\n",
    "\n",
    "# Identify and store duplicates (for inspection)\n",
    "df_duplicate_products = df[df.duplicated(subset=['brand', 'price', 'discount', 'avg_rating', 'total_ratings'])]\n",
    "\n",
    "# Drop duplicates to keep only unique products\n",
    "df = df.drop_duplicates(subset=['brand', 'price', 'discount', 'avg_rating', 'total_ratings'])\n",
    "\n",
    "# DataFrame for unavailable or failed URLs\n",
    "df_unavailable_products = pd.DataFrame(unavailable_products, columns=['link'])\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. Print summary statistics\n",
    "# ------------------------------------------------------------\n",
    "print(\"Total product pages scrapped: \", len(all_product_links))\n",
    "print(\"Final Total Products: \", len(df))\n",
    "print(\"Total Unavailable Products : \", len(df_unavailable_products))\n",
    "print(\"Total Duplicate Products: \", len(df_duplicate_products))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. Save the data to CSV files\n",
    "# ------------------------------------------------------------\n",
    "df.to_csv('flipkart_product_data.csv', index=False)\n",
    "df_unavailable_products.to_csv('unavailable_products.csv', index=False)\n",
    "df_duplicate_products.to_csv('duplicate_products.csv', index=False)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8. Close the browser session and log end time\n",
    "# ------------------------------------------------------------\n",
    "driver.close()\n",
    "session_end_time = datetime.now().time()\n",
    "print(f\"Session End Time: {session_end_time} ---------------------------> \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d90f7d-f3f4-4dfa-981b-770b03202340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:venvai]",
   "language": "python",
   "name": "conda-env-venvai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
